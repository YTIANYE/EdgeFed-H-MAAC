# 未完成的计划

计划 年龄和任务数做比



#### 计划

程序计算的平均年龄和手动计算的平均年龄结果不同的原因是什么？

实验8.1



#### 计划

查看下单目标任务数时和双目标时，年龄的优化情况。实验7.4

#### 计划

了解别人多目标是如何联合优化的

了解不同采样方式有什么作用

#### 计划

当前任务数，以最近平均任务数替代

#### 计划

sample方式对最近平均任务数有没有作用。

验证：sample方式的修改可能仅在年龄处理上有优势。





#### 计划

根号下new_rewards_age[i] / MAX_EPOCH ？开方？

#### 计划

如果关于reward那部分，就只以当前完成的任务数作为reward_average呢，而不是最近平均完成的任务数或者总平均任务数，？波动应该会很大，类似200到16的变化。但是和年龄联合在一起，并且与对比算法进行对比，是不是就不会很大了？

#### 计划

研究一下MAAC不稳定原因: 可能是没有一个共同的reward导致的，H-MAAC和AC都是共同的reward

#### 计划

研究惩罚和奖励的区别

我的loss加没加正则化？





#### 计划（未做）

```
实验6: new_reward 方式五
最近平均年龄
new_rewards = new_rewards_average_task[i] / self.agent_num * weight_average  + 
			(1 -  new_rewards_average_age[i] / epoch )* weight_age
weight_age + weight_average = 1, weight_age = 0.5

实验7：当前任务个数和当前年龄直接加权求和

实验：不断记录最大的年龄做分母（age的权重还是动态变化的，这样可能并不好）
new_rewards = new_rewards_average[i] / self.agent_num * weight_average  + (1 -  new_rewards_age[i] / max_age ) * weight_age   
weight_age + weight_average = 1
```








# 已完成的计划

#### 计划10.19

实验8.3：new_rewards方式5.2：任务数平方

**new_rewards = (new_rewards_average[i] * new_rewards_average[i]) * weight_average - new_rewards_age[i] * weight_age**

weight_age = 1    weight_average = 1



#### 计划10.18

最近平均任务数

sample方式二

**new_rewards = new_rewards_average[i] * weight_average - new_rewards_age[i] * weight_age**

weight_age = weight_average = 1



#### 计划10.18

去归一化、年龄成系数与 任务数相减，对比episode_reward_age在单目标和双目标的值。



#### 计划10.16

在record中，添加episode_reward ,step_reward 数据记录



#### 推导（new_rewards 的 演化）

```
new_rewards 的 演化

new_rewards = (new_rewards_average[i] / self.agent_num * 1.0 - new_rewards_age[i] / epoch * 1.0 + 1.0) / 2

new_rewards = new_rewards_average[i] / self.agent_num * 0.5  -  new_rewards_age[i] / epoch * 0.5  +  0.5

new_rewards = new_rewards_average[i] / self.agent_num * w  -  new_rewards_age[i] / epoch * (1 - w)  +  (1 - w)

new_rewards = new_rewards_average[i] / self.agent_num * w  + (1 -  new_rewards_age[i] / epoch ) * (1 - w))
```



#### 计划10.10

单目标优化，关于年龄，对比sample方式的改进效果。

```
agent_num = 8
MAX_EPOCH = 3000
epoch_num = 200

H-MAAC  MAAC  AC

sample方式二
sensor_nums = [40, 50, 60, 70, 80]
sample方式一
sensor_nums = [40, 50, 60, 70, 80]
```



#### 计划10.06

测试new_rewards方式三时，sample仅在优化年龄问题时，是否起作用（w = 1）

(研究时，w = 1或0就变成了了单目标的优化)

```
实验5.2：reward侧重年龄下，sample方式对比
weight_age + weight_average = 1, weight_age = 0.1

sample方式二、方式一
H-MAAC、MAAC、AC
weight_age = 0.5
weight_age = 0.1
weight_age = 0.9
weight_age = 1.0
```



#### 计划10.03

```
agent_num = 8
MAX_EPOCH = 3000
epoch_num = 200

实验5： new_rewards 方式三
new_rewards = new_rewards_average[i] / self.agent_num * weight_average  + (1 -  new_rewards_age[i] / MAX_EPOCH )* weight_age   
weight_age + weight_average = 1

实验5.1：
weight_age = 0.5

AC
H-MAAC
MAAC

实验5.2：reward侧重年龄下，sample方式对比
weight_age + weight_average = 1, weight_age = 0.1
MAAC

H-MAAC
sample方式二

weight_age = 0.5
weight_age = 0.1
weight_age = 0.9

实验6：reward方式四1:大于300的置为1
new_rewards = (new_rewards_average[i] / self.agent_num * weight_average + (1 - reward_age) * weight_age)
reward_age = min(1, new_rewards_age[i] / (MAX_EPOCH / 10))

weight_age = 0.9
H-MAAC(1)
MAAC(3)
AC(1)

weight_age = 0.5
H-MAAC(1)
MAAC(3)
AC(1)

未实验
实验5.4：reward方式三改进2：除以1000；应该意义不大
new_rewards = (new_rewards_average[i] / self.agent_num * weight_average + (1 - reward_age) * weight_age)
reward_age = min(1, new_rewards_age[i] / 1000)
weight_age = 0.5

```

#### 

#### 计划9.28

改善联合优化的reward

```
agent_num = 8
MAX_EPOCH = 3000
epoch_num = 200

实验3
new_rewards = (new_rewards_average[i] / self.agent_num * 1.0 - new_rewards_age[i] / epoch * 1.0)

H-MAAC
sensor_nums = [60, 60]

实验4.1：方式二限定范围[0,1]
new_rewards = (new_rewards_average[i] / self.agent_num * 1.0 - new_rewards_age[i] / epoch * 1.0 + 1.0) / 2

H-MAAC
sensor_nums = [60, 60, 60, 60, 60]
MAAC
sensor_nums = [60, 60, 60, 60, 60]
AC
sensor_nums = [60, 60, 60, 60, 60]

为了单独观察age和average，再跑一遍

H-MAAC
sensor_nums = [60]
MAAC
sensor_nums = [60, 60, 60, 60, 60]		# 验证5次曲线是否一样
AC
sensor_nums = [60]	# 修改了sample方式，进行对比

实验4.2：研究epoch的影响
MAX_EPOCH = 10000

H-MAAC
sensor_nums = [60]

实验4.3：研究epoch_num的影响
epoch_num = 16

H-MAAC
sensor_nums = [60]
MAAC
sensor_nums = [60]
AC
sensor_nums = [60]

实验4.4：研究sample方式的影响

方式二
AC
sensor_nums = [60]

方式一
H-MAAC
sensor_nums = [60, 60]
MAAC
sensor_nums = [60, 60]
AC
sensor_nums = [60, 60]
```



#### 计划9.26

实现联合优化

实验

```
new_rewards = [(new_rewards_average[i] / 8.0 * 1.0 - new_rewards_age[i] / 100.0 * 1.0)
agent_num = 8
MAX_EPOCH = 3000

实验1
epoch_num = 16

H-MAAC
sensor_nums = [60, 60, 60, 60, 60]
MAAC
sensor_nums = [60, 60, 60, 60, 60]

实验2
epoch_num = 200

H-MAAC
sensor_nums = [60, 60, 60, 60, 60]
MAAC
sensor_nums = [60]
```



#### 计划9.24

跑没有联合学习的算法，总平均任务数和最近平均任务数

###### 实验

```
reward：最近完成的平均任务数
agent_num = 8
MAX_EPOCH = 3000
epoch_num = 200
FL = False

sensor_nums = [60, 60, 60, 60, 60, 60, 60, 60, 60, 60]
MAAC
```



#### 计划9.23

最近平均任务数比较MAAC 和 AC

###### 实验

```
reward：最近完成的平均任务数
agent_num = 8
MAX_EPOCH = 3000
epoch_num = 200

sensor_nums = [60, 60, 60, 60, 60, 60, 60, 60, 60, 60]
MAAC

sensor_nums = [60, 60, 60, 60, 60, 60, 60, 60, 60, 60]
AC
```



#### 计划9.22

##### 实验：

```
MAAC
sample方式二
sensor_nums = [60, 60, 60, 60, 60]
reward：平均任务数目
```

